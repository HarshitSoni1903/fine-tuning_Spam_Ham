# SMS Spam Detection with LLM Fine-Tuning

## Overview

I have built an **AI agent** to classify SMS messages as **spam or ham (not spam)**.  This fulfils the following criteria:

- Use of an **open-source Large Language Model (LLM)**
- Training on a **public dataset**
- A hybrid **inference engine** that combines LLM outputs with rule-based checks for improved reliability
- **Two distinct fine-tuning approaches**, compared side by side

---

## Models & Dataset

- **Dataset:** [UCI SMS Spam Collection](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)  
  A publicly available dataset of SMS messages labeled as spam or ham.

- **Base Model:** [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama)  
  A lightweight, open-source LLM suitable for conversational tasks.

---

## Fine-Tuning Approaches

### 1️. QLoRA
- Fine-tunes the LLM using **QLoRA** (4-bit quantized LoRA) for parameter-efficient training.
- Trains additional low-rank adapters on top of frozen quantized weights.

### 2️. Prompt Tuning
- Keeps the base LLM entirely frozen.
- Learns a set of **soft virtual tokens** that steer the model’s responses.

---

## Inference Engine

The agent performs two steps for each input message:

1. **LLM Classification:** Asks the LLM if the SMS is spam or ham.
2. **Rule-Based Layer:** Checks for strong spam patterns (e.g., suspicious keywords, excessive capitalization) to adjust confidence.

---

## Evaluation

- Tested on a held-out test split from the dataset.
- Compared using:
  - **Accuracy**
  - **Precision**
  - **Recall**
  - **F1 Score**


| Model     | Accuracy | Precision | Recall | F1 Score |
|-----------|----------|-----------|--------|----------|
| **QLoRA**     | **0.95** | **0.87**  | **0.70** | **0.78** |
| Prompt    | 0.78     | 0.32      | 0.57   | 0.41     |

---

## Conclusion & Recommendation
- The QLoRA fine-tuning approach achieved substantially better performance:
  - Higher **accuracy (95% vs 78%)**
  - Much stronger **precision (87% vs 32%)**, meaning it makes far fewer false spam predictions.
  - Better **recall (70% vs 57%)**, capturing more actual spam messages.
  - A balanced **F1 score (0.78 vs 0.41)**, showing overall stronger spam detection.
- The responses generated by prompt-tuned models were more nonsensical.

**Best approach:** Use the QLoRA fine-tuned model for this task, as it strikes a better balance between correctly identifying spam and avoiding false positives, leading to stronger overall performance on unseen data.


---

## How to Run

### Environment setup

This project is designed to run on **Google Colab** with a **T4 GPU**.  
(You can ensure this by selecting: `Runtime > Change runtime type > T4 GPU`.)

---

### Install dependencies

Run the **first cell** in the notebook to install required libraries. Ensure that you restart the environment after installation is complete.


## Future work
- Use larger LLMs for potentially higher accuracy.
- Use a custom classification head instead of a purely generative approach.
- Add more robust keyword or regex-based heuristics.
- Expand dataset beyond SMS (e.g., email or social media spam).
- Train on a larger dataset, this was a significantly small dataset, thus less to be learnt by the modern models.
